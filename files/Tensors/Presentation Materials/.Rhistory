# growth data
girls <- read.table("C:\\Users\\dakot\\Dropbox\\UCONN Course Materials\\UconnS19\\EPSY6619 - Advanced Modeling\\Tensor Decompositions\\Workbook\\girlsgrowthcurves.dat")
names(girls) <- c("weight","length","crrump",
"head","chest","arm","calf",
"pelvis")
# Mode-1 (Persons) - 30 french girls
# Mode-2 (Variables)
# Mode-3 (Age or Time) - Age 4 to Age 15
# data is currently in this form
# j=1,.,J=8
# |-----|i=1
# |     |i=2
# |     |..     k= 1
# |     |..
# |_____|i=I=30
# |-----|i=1
# |     |i=2
# |     |..     k= 2
# |     |..
# |_____|i=I=30
# |-----|i=1
# |     |i=2
# |     |..     k=12
# |     |..
# |_____|i=I=30
# three-way data has is in this form X = x(i,j,k)
#                  |-----|i=1
#             |-----|    |i=2
#        |-----|    |    |..
#        |     |    |    |..
#        |     |    |____|i=I=30    k=K=12
#        |     |____|            k=2
#        |_____|              k=1
#     j=1,.,J=8
# We need to reshape this data as a multiway array
# convert data into list of dataframes
list <- split(girls, rep(1:12, each = 30))
### Function to convert a list of dataframes to a 3D array
### All objects in the list will be dataframes with identical column headings.
list2ary = function(input.list){  #input a list of dfs
rows.cols <- dim(input.list[[1]]) #dim of all dataframes, assumed fixed
sheets <- length(input.list) # number of dataframes
output.ary <- array(unlist(input.list), dim = c(rows.cols, sheets))
colnames(output.ary) <- colnames(input.list[[1]])
row.names(output.ary) <- row.names(input.list[[1]])
return(output.ary)    # output as a 3-D array
}
# convert list of data frames to an multiway array or tensor
girls.ary <- list2ary(list)
# You can check that this worked (uncomment and run)
# table(growth.ary[,,1] == growth[1:30,])
# table(growth.ary[,,12] == growth[331:360,])
# dimesions multidimensional array
dim(girls.ary)
# age 4 values for first 6 girls
girls.ary[1:6,,1]
library(rrcov3way)
## Center the data in mode 1 and find the "average girl"
center.girls <- do3Scale(girls.ary, center=TRUE, scale = TRUE, only.data=FALSE,
center.mode = c("A"), scale.mode = c("B"))
X <- center.girls$x
center <- center.girls$center
average.girl <- as.data.frame(matrix(center, ncol=8, byrow=TRUE))
colnames(average.girl) <- colnames(girls.ary)
## Divide these variables by 10 to reduce their range
average.girl$weight <- average.girl$weight/10
average.girl$length <- average.girl$length/10
average.girl$crrump <- average.girl$crrump/10
par(xpd = T, mar = par()$mar + c(0,0,0,7))
p <- ncol(average.girl)
plot(rownames(average.girl), average.girl[,1], ylim=c(min(average.girl),
max(average.girl)), type="n", xlab="Age", ylab="")
for(i in 1: p)
{
lines(rownames(average.girl), average.girl[,i], lty=i, col=i)
points(rownames(average.girl), average.girl[,i], pch=i, col=i)
}
legend <- colnames(average.girl)
legend("topright", inset=c(-0.26,0), xpd=TRUE, legend=legend, col=1:p, lty=1:p, pch=1:p, title="Variables")
shiny::runApp('C:/Users/dakot/Dropbox/Data Science Course/ShinyApp-Support-Vector-Machines-master/SVM')
library(topicmodels)
?LDA
knitr::opts_chunk$set(echo = TRUE)
# read data with rio package. save to tweets object. (1)
tweets <- rio::import("C:\\Users\\dakot\\Dropbox\\Methods Note\\equity.csv", encoding="UTF-8")
# names for variables in dataset (2)
names(tweets) <- c("date","tweet", "retweets", "favs",
"followers", "follows")
# remove duplicated tweets (3)
tweets = tweets[!duplicated(tweets$tweet),]
# convert to tibble (4)
tweets <- dplyr::as_tibble(tweets)
# create unique id for each tweet (5)
tweets <- dplyr::mutate(tweets, id = rownames(tweets))
# rename id to tweetid (6)
names(tweets) <- c("date","tweet", "retweets", "favs",
"followers", "follows", "tweetid")
# display data in tibble format
tweets
# cleaning strings function
# the function executes a set of procedures on a set of texts
textprocess <- function(texts = texts){
# import libraries
library(stringr)
library(textclean)
# manipulate texts
texts = str_to_lower(texts) # tolower (1)
texts = gsub('[[:punct:] ]+',' ',texts) # remove punctuation (2)
texts = str_replace_all(texts, "[^[:alnum:]]", " ") # replace alphanumbers with space (3)
texts = strip(texts) # fnc to remove unwanted symbols (4)
texts = gsub(" *\\b(?<!-)\\w{1,2}(?!-)\\b *", " ", texts, perl=T) # removing any special characters (5)
# return texts
texts
}
# clean data by running function
tweets$tweet<- textprocess(texts = tweets$tweet)
# display data
tweets
library(dplyr)
# tokenize data with unnest_tokens (1 word tokens)
# we will create bigrams using the tokenized unigram data
tweet_unigrams <- tweets %>%
tidytext::unnest_tokens(word, tweet, token = "ngrams", n =1)
tweet_unigrams
# custom stopwords
# note there is a default dictionary used here (see ?stop_words)
library(tidytext)
custom_stop_words_tweet <- bind_rows(data_frame(word = c("equity"), lexicon = c("custom")), stop_words)
# remove stopwords
tweet_unigrams_nostop <- tweet_unigrams %>%
anti_join(custom_stop_words_tweet)
library(ggplot2)
tweet_unigrams_nostop %>%
count(word, sort = TRUE) %>%
filter(n > 60) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
ylab("Frequency") +
coord_flip()
library(dplyr)
library(tidytext)
# counts words by id and sort by greatest n (1)
tweet_unigrams_word_counts  <- tweet_unigrams_nostop %>%
count(tweetid, word, sort = TRUE) %>%
ungroup()
# creating total words used by id (2)
tweet_unigrams_total_counts <- tweet_unigrams_word_counts %>%
group_by(tweetid) %>%
summarize(total = sum(n))
# joining two tables by id (3)
tweet_unigrams_counts <- left_join(tweet_unigrams_word_counts, tweet_unigrams_total_counts)
# cast into a Document-Term Matrix (4)
tweet_unigrams_words_dtm <- tweet_unigrams_counts  %>%
tidytext::cast_dtm(tweetid, word, total)
tweet_unigrams_words_dtm
library(ggplot2)
library(dplyr)
library(topicmodels)
# four topics unigram (1)
tweet_unigrams_lda <- LDA(tweet_unigrams_words_dtm, method = "VEM", k = 2, control = list(seed = 1234))
# beta matrix P(word|topic) (2)
tweet_word_topic <- tidy(tweet_unigrams_lda, matrix = "beta")
# top terms
tweet_lda_top_terms <- tweet_word_topic %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# covert to factor
tweet_lda_top_terms$topic <- factor(tweet_lda_top_terms$topic)
# plot top 15 Terms in each LDA topic
tweet_lda_top_terms %>%
mutate(term = reorder(term, beta)) %>%
group_by(topic, term) %>%
arrange(desc(beta)) %>%
ungroup() %>%
mutate(term = factor(paste(term, topic, sep = "__"),
levels = rev(paste(term, topic, sep = "__")))) %>%
ggplot(aes(term, beta, fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
labs(title = "Top 15 terms in each of the two LDA topics",
x = NULL, y = expression(beta)) +
facet_wrap(~ topic, ncol = 2, scales = "free")
tweers_unigrams_lda
tweet_unigrams_lda
names(tweet_unigrams_lda)
tweet_unigrams_lda
?LDA
library(ggplot2)
library(dplyr)
library(topicmodels)
# four topics unigram (1)
tweet_unigrams_lda <- LDA(tweet_unigrams_words_dtm, method = "VEM", k = 2, control = list(seed = 1234, estimate.alpha = T, estimate.beta = T))
# beta matrix P(word|topic) (2)
tweet_word_topic <- tidy(tweet_unigrams_lda, matrix = "beta")
# top terms
tweet_lda_top_terms <- tweet_word_topic %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# covert to factor
tweet_lda_top_terms$topic <- factor(tweet_lda_top_terms$topic)
# plot top 15 Terms in each LDA topic
tweet_lda_top_terms %>%
mutate(term = reorder(term, beta)) %>%
group_by(topic, term) %>%
arrange(desc(beta)) %>%
ungroup() %>%
mutate(term = factor(paste(term, topic, sep = "__"),
levels = rev(paste(term, topic, sep = "__")))) %>%
ggplot(aes(term, beta, fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
labs(title = "Top 15 terms in each of the two LDA topics",
x = NULL, y = expression(beta)) +
facet_wrap(~ topic, ncol = 2, scales = "free")
library(ldatuning)
library(ldatuning)
install.packages("ldatuning")
?ldatuning
library(ldatuning)
?ldatuning
